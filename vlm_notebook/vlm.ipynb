{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "import requests\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    print(\"MPS backend is not available. Please check your PyTorch installation or hardware compatibility.\")\n",
    "    sys.exit(1)  # Exit the script\n",
    "\n",
    "# Use the MPS device\n",
    "device = torch.device(\"mps\")\n",
    "print(\"Using MPS device for computation.\")\n",
    "\n",
    "# Load the model with MPS support\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float32  # Use float32 for compatibility with MPS\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# Load the image from a URL\n",
    "url = \"https://www.health.com/thmb/eD_xfu60sILf0i_O9gPn8pbpWkM=/750x0/filters:no_upscale():max_bytes(150000):strip_icc()/Health-Atopic-Dermatitis-48.flexural-eczema22-96dcdcccdb154bfaa44b70f52e6ecc1c.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "# Prepare the input message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are given an image related to some skin problems. Explain the cause and cure with suitable medications in detail.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    text=[text_prompt],\n",
    "    images=[image],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Move inputs to the MPS device\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode and display the result\n",
    "result = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"mdwiratathya/ROCO-radiology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Specify cache directory\n",
    "cache_dir = \"/Users/sambhavjha/Downloads/vlm_notebook/huggingface_cache\"\n",
    "dataset_path = \"/Users/sambhavjha/Downloads/vlm_notebook/datasets/ROCO-radiology\"\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"mdwiratathya/ROCO-radiology\", cache_dir=cache_dir)\n",
    "\n",
    "# Save each split to disk\n",
    "ds[\"train\"].save_to_disk(f\"{dataset_path}/train\")\n",
    "ds[\"validation\"].save_to_disk(f\"{dataset_path}/validation\")\n",
    "ds[\"test\"].save_to_disk(f\"{dataset_path}/test\")\n",
    "\n",
    "print(f\"Dataset saved successfully at {dataset_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Check if MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    raise EnvironmentError(\"MPS backend is not available. Ensure you're using a macOS system with M1/M2 GPU and PyTorch is installed correctly.\")\n",
    "\n",
    "# Set the device to MPS\n",
    "device = torch.device(\"mps\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float32,  # Use float32 for MPS compatibility\n",
    "    device_map=None  # Avoid auto device mapping\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"attn.qkv\", \"attn.proj\",  # Vision attention layers\n",
    "        \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\"  # Decoder layers\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Move LoRA model to MPS\n",
    "model = model.to(device)\n",
    "\n",
    "# Check applied LoRA modules\n",
    "print(\"Applied LoRA modules:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# === DEVICE SETUP ===\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === LOAD DATASET ===\n",
    "cache_dir = \"/Users/sambhavjha/Downloads/vlm_notebook/huggingface_cache\"  # Replace with a valid path\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"mdwiratathya/ROCO-radiology\", cache_dir=cache_dir, streaming=True)\n",
    "\n",
    "# === INITIALIZE PROCESSOR ===\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# === DEFINE PREPROCESSING FUNCTION ===\n",
    "def preprocess_function(example):\n",
    "    \"\"\"\n",
    "    Preprocess a single example for incremental processing.\n",
    "    \"\"\"\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),  # Resize image\n",
    "        ToTensor()  # Convert to tensor\n",
    "    ])\n",
    "\n",
    "    # Convert grayscale image to RGB if needed\n",
    "    image = example[\"image\"]\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Resize and convert to tensor\n",
    "    image = transform(image)\n",
    "\n",
    "    # Process inputs with the processor\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=example[\"caption\"],  # Corresponding caption\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(0).to(device),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(0).to(device),\n",
    "        \"pixel_values\": inputs[\"pixel_values\"].squeeze(0).to(device),\n",
    "    }\n",
    "\n",
    "# === PROCESS DATASETS ===\n",
    "def process_stream(dataset_stream, batch_size=16):\n",
    "    \"\"\"\n",
    "    Incrementally process the dataset in batches to handle the full dataset.\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    for i, example in enumerate(dataset_stream):\n",
    "        batch.append(example)\n",
    "\n",
    "        # Process in batches\n",
    "        if len(batch) == batch_size:\n",
    "            yield [preprocess_function(ex) for ex in batch]\n",
    "            batch = []  # Clear the batch\n",
    "\n",
    "            # Periodically free up memory\n",
    "            gc.collect()\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "            # Log progress\n",
    "            print(f\"Processed {i + 1} samples...\")\n",
    "\n",
    "    # Process any remaining examples\n",
    "    if batch:\n",
    "        yield [preprocess_function(ex) for ex in batch]\n",
    "\n",
    "# === PROCESS FULL DATASETS ===\n",
    "print(\"Processing train dataset...\")\n",
    "train_processed = []\n",
    "for processed_batch in process_stream(ds[\"train\"], batch_size=16):\n",
    "    train_processed.extend(processed_batch)\n",
    "\n",
    "print(\"Processing validation dataset...\")\n",
    "val_processed = []\n",
    "for processed_batch in process_stream(ds[\"validation\"], batch_size=16):\n",
    "    val_processed.extend(processed_batch)\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 epochs\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            pixel_values=batch[\"pixel_values\"],\n",
    "            labels=batch[\"input_ids\"]  # Teacher forcing\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if MPS is available for Mac M1/M2\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and processor\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.float32,  # Use float32 for MPS compatibility\n",
    "    device_map=None\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"visual.blocks.*.attn.qkv\", \"visual.blocks.*.attn.proj\",  # Vision attention layers\n",
    "        \"model.layers.*.self_attn.q_proj\", \"model.layers.*.self_attn.k_proj\", \n",
    "        \"model.layers.*.self_attn.v_proj\", \"model.layers.*.self_attn.o_proj\",  # Decoder self-attention layers\n",
    "        \"model.layers.*.mlp.gate_proj\", \"model.layers.*.mlp.up_proj\", \"model.layers.*.mlp.down_proj\"  # Decoder MLP layers\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Applied LoRA modules:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
